{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5878857-2257-40a8-9c79-9d57b50047cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.2 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from pyspark==3.4.2) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc6d5d1-70af-47ce-8129-731035913b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (7.2.7)\n",
      "Requirement already satisfied: pycryptodome in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from minio) (3.20.0)\n",
      "Requirement already satisfied: certifi in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from minio) (2024.7.4)\n",
      "Requirement already satisfied: argon2-cffi in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: urllib3 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from minio) (2.2.2)\n",
      "Requirement already satisfied: typing-extensions in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from minio) (4.12.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9aef6a-157f-4e54-bee7-642b38391de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark==2.4.0 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: pyspark<3.5.0,>=3.4.0 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from delta-spark==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from delta-spark==2.4.0) (8.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0) (3.19.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages (from pyspark<3.5.0,>=3.4.0->delta-spark==2.4.0) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f1fb93-d5ef-4a8f-b9e2-6a891863f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from minio import Minio\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ea502d-c1e5-45a9-9325-d1f533b69f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 19:14:16 WARN Utils: Your hostname, DESKTOP-CV9TLA8 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/05 19:14:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hacs/.cache/pypoetry/virtualenvs/pyspark-delta-gPM1PAZH-py3.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hacs/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hacs/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b6824135-4496-45e3-8a71-5a24718ef24e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 165ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b6824135-4496-45e3-8a71-5a24718ef24e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "24/09/05 19:14:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").appName(\"csvToParquet\")\n",
    "    #HADOOP\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #DELTA\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars\", \"jars/aws-java-sdk-bundle-1.11.1026.jar, \\\n",
    "            jars/hadoop-aws-3.2.0.jar\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970b86ee-ce00-4329-9406-fbb2f79ffefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=Minio(\"localhost:9000\",\n",
    "             access_key=\"minioadmin\",\n",
    "             secret_key=\"minioadmin\",\n",
    "             secure=False\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d0aa29-31f4-42af-ad39-2c1308a08f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects=client.list_objects('raw', prefix='CAPES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410ff462-641b-4973-8ea8-32d0b88848fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects=client.list_objects('raw', prefix='CAPES/')\n",
    "csvs = []\n",
    "for object in objects:\n",
    "    csvs.append(object.object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66479d4-8350-457c-8b69-eb60c31153c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO PARA LISTAR AS COLUNAS IGUAIS DOS DOIS DATAFRAMES\n",
    "def equalslist(df1, df2):\n",
    "    return list(set(df1.columns).intersection(set(df2.columns)))\n",
    "\n",
    "#FUNÇÃO PARA JUNTAR DOIS DATAFRAMES, ESPECIFICANDO A KEY USADA E O MÉTODO DE JUNÇÃO \n",
    "def dfjoin(dataframe1, dataframe2, key, typejoin):\n",
    "    \n",
    "    equals = equalslist(dataframe1,dataframe2)\n",
    "    equals.remove(key)\n",
    "\n",
    "    #USANDO UM DATAFRAME AUXILIAR PARA NÃO MODIFICAR O DATAFRAME RAIZ CARREGADO\n",
    "    dataframe3 = dataframe2\n",
    "    for column in equals:\n",
    "        dataframe3 = dataframe3.drop(column)\n",
    "        \n",
    "    dataframe_join = dataframe1.join(dataframe3, key, typejoin)\n",
    "\n",
    "    #RETIRANDO AS LINHAS EM QUE A KEY É NULA\n",
    "    dataframe_join = dataframe_join.dropna(subset=[key])\n",
    "    return dataframe_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde116eb-c10e-4f29-b4d8-a06ca8d37866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - CAPES/2021-a-2024-autor-da-producao-intelectual-de-programas-de-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "1 - CAPES/2021-a-2024-catalogo-de-teses-e-dissertacoes-brasil.csv\n",
      "2 - CAPES/2021-a-2024-cursos-da-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "3 - CAPES/2021-a-2024-detalhes-da-producao-intelectual-bibliografica-de-programas-de-pos-graduacao.csv\n",
      "4 - CAPES/2021-a-2024-detalhes-da-producao-intelectual-tecnica-de-programas-de-pos-graduacao-stricto-sensu.csv\n",
      "5 - CAPES/2021-a-2024-discentes-da-pos-graduacao-stricto-sensu-do-brasil.csv\n",
      "6 - CAPES/2021-a-2024-docentes-da-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "7 - CAPES/2021-a-2024-financiadores-de-projetos-dos-programas-de-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "8 - CAPES/2021-a-2024-membros-de-projetos-dos-programas-de-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "9 - CAPES/2021-a-2024-producao-intelectual-de-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "10 - CAPES/2021-a-2024-programas-da-pos-graduacao-stricto-sensu-no-brasil.csv\n",
      "11 - CAPES/2021-a-2024-projetos-dos-programas-de-pos-graduacao-stricto-sensu-no-brasil.csv\n"
     ]
    }
   ],
   "source": [
    "#LISTANDO OS DATAFRAMES PRESENTES NO BUCKET RAW DO MinIO\n",
    "count = 0\n",
    "for csv in csvs:\n",
    "    print(f'{count} - {csv}')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5951cf16-2425-4cdf-9010-a613af9f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARREGANDO TODOS OS DATAFRAMES\n",
    "bucket_name = 'raw'\n",
    "autores = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[0]}')\n",
    "teses = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[1]}')\n",
    "cursos = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[2]}')\n",
    "bibliografica = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[3]}')\n",
    "tecnica = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[4]}')\n",
    "discentes = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[5]}')\n",
    "docentes = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[6]}')\n",
    "financiadores = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[7]}')\n",
    "membros = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[8]}')\n",
    "producoes = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[9]}')\n",
    "programas = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[10]}')\n",
    "projetos = spark.read.format('csv').option('header', 'true').option('sep', ';').load(f's3a://{bucket_name}/{csvs[11]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d62f881-e323-40b7-a336-050c058da870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO O BUCKET SE ELE NÃO EXISTIR\n",
    "bucket_name=\"bronze\"\n",
    "found = client.bucket_exists(bucket_name)\n",
    "if not found:\n",
    "    client.make_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1836b282-1ba8-488c-acef-701a68656659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO DE CONVERSAO PARA DELTA\n",
    "def df_to_parquet(df, name_df):\n",
    "    (df.write \n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(f\"delta/{name_df}\")\n",
    "    )\n",
    "#FUNÇÃO PARA ENVIAR O DATAFRAME PARA O MinIO, CONVERTENDO EM DELTA NO PROCESSO E SALVANDO UMA VERSÃO LOCAL PARA PRESERVAR O LOG\n",
    "def df_to_minio(df, bucket_name, name_df):\n",
    "    #USANDO A FUNÇÃO DE CONVERSÃO EM DELTA, EM QUE A MESMA SALVA NA PASTA LOCAL DELTA COM O NOME ESPECIFICADO\n",
    "    df_to_parquet(df, name_df)\n",
    "\n",
    "    #ANDANDO PELA PASTA ESPECIFICA DO ARQUIVO DELTA E ENVIANDO-O PARA O BUCKET ESPECIFICADO\n",
    "    for root, _, files in os.walk(f\"delta/{name_df}\"):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            object_name = os.path.relpath(file_path, f\"delta/{name_df}\")\n",
    "            object_name = os.path.join(name_df, object_name)\n",
    "            client.fput_object(bucket_name, object_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904d7ef2-d210-41d2-b063-a9193421bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO O PRIMEIRO DATAFRAME: PROJETOS + MEMBROS\n",
    "projetos_membros_ufma = dfjoin(projetos, membros, \"ID_PROJETO\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5547aece-c27b-457e-bed9-7b85211fcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTRO:UFMA PARA FILTRAR OS DISCENTES E DOCENTES DOS PROJETOS\n",
    "projetos_membros_ufma = projetos_membros_ufma.filter(F.col(\"SG_ENTIDADE_ENSINO\") == (\"UFMA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71c02c9-5921-4d0b-9bad-85bbd3dbbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTRO:%COMPUTA% PARA FILTRAR A AREA DE ATUAÇÃO PARA CIENCIA DA COMPUTAÇÃO\n",
    "projetos_membros_ufma = projetos_membros_ufma.filter(F.col(\"NM_AREA_AVALIACAO\").like(\"%COMPUTA%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1be11da-ed62-4064-8496-a97b6223ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO O DATAFRAME DE PROGRAMA DOS PROJETOS DOS MEMBROS DA UFMA\n",
    "programas_projetos_membros = dfjoin(projetos_membros_ufma, programas, \"CD_PROGRAMA_IES\", \"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2258d271-dc28-4d70-ae67-b184a5afa46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programas_projetos_membros.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70d99279-7907-412f-8e8c-155077444644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIVIDINDO O DATAFRAME EM DISCENTES/DOCENTES\n",
    "#CRIANDO O DATAFRAME DE PROGRAMAS DOS PROJETOS DOS DISCENTES\n",
    "programas_projetos_discentes = dfjoin(programas_projetos_membros, discentes, \"ID_PESSOA\", \"inner\")\n",
    "programas_projetos_discentes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "216e3467-0958-4d1b-ba13-5ad1160627e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E RETIRANDO AS DUPLICATAS\n",
    "programas_projetos_discentes = programas_projetos_discentes.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5a110c21-c500-435a-9c0e-b6f73cd2105b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CRIANDO O DATAFRAME DE PROGRAMAS DOS PROJETOS DOS DOCENTES\n",
    "programas_projetos_docentes = dfjoin(programas_projetos_membros, docentes, \"ID_PESSOA\", \"inner\")\n",
    "programas_projetos_docentes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b5510a5-069c-4cec-98d3-a7b802078245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E RETIRANDO AS DUPLICATAS\n",
    "programas_projetos_docentes = programas_projetos_docentes.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ed51bee-083f-49b0-b879-0efca91c86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO OS DATAFRAMES DOS DISCENTES/DOCENTES PARA TECNICA/BIBLIOGRAFICA\n",
    "\n",
    "#CRIANDO O DATAFRAME DE TECNICA DOS PROGRAMAS DOS PROJETOS DOS DISCENTES\n",
    "detalhes_tecnica_discentes = dfjoin(programas_projetos_discentes, tecnica, \"CD_PROGRAMA_IES\", \"inner\")\n",
    "\n",
    "#CRIANDO O DATAFRAME DE TECNICA DOS PROGRAMAS DOS PROJETOS DOS DOCENTES\n",
    "detalhes_tecnica_docentes = dfjoin(programas_projetos_docentes, tecnica, \"CD_PROGRAMA_IES\", \"inner\")\n",
    "\n",
    "#CRIANDO O DATAFRAME DE BIBLIOGRAFICA DOS PROGRAMAS DOS PROJETOS DOS DISCENTES\n",
    "detalhes_bibliografica_discentes = dfjoin(programas_projetos_discentes, bibliografica, \"CD_PROGRAMA_IES\", \"inner\")\n",
    "\n",
    "#CRIANDO O DATAFRAME DE BIBLIOGRAFICA DOS PROGRAMAS DOS PROJETOS DOS DOCENTES\n",
    "detalhes_bibliografica_docentes = dfjoin(programas_projetos_docentes, bibliografica, \"CD_PROGRAMA_IES\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a658412-9086-4c88-938b-890c0fdf2eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
